## 지도 학습
데이터(특성+타겟) → 모델 → 데이터 예측

+ ### 회귀: 연속형 데이터
  임의의 어떤 숫자를 예측하는 문제
  ```
  가중치 = 기울기(계수) 
  편향 = 절편
  
  특성 - 예측 변수
  타겟 - 반응 변수
  
  가설 함수: x축-독립변수, y축-종속변수
  비용 함수: x축-가중치, y축-오차

  최적화: 비용 함수에서 가중치(미분=0)를 탐색하는 과정
  
  예) 경제 성장률, 배달 도착 시간, ...
  ```
  >K-최근접 이웃 회귀(K-Nearest Neighbor Regression)
  >```
  >데이터에서 거리가 가까운 K개 샘플들의 평균값으로 예측
  >
  >최적화
  >- 과대적합 → K 증가 → 모델 단순
  >- 과소적합 → K 감소 → 모델 복잡
  >
  >· 단점: 훈련 데이터 범위 밖의 샘플 예측X
  >```
  >
  >선형 회귀(Linear Regression)
  >```
  >특성(1개)과 타겟 사이의 관계를 가장 잘 나타내는 선형방정식(직선)으로 예측
  >예) y = ax + b
  >
  >- 가설 함수: 직선
  >- 비용 함수: 최소제곱법(Ordinary Least Squares, OLS)
  >
  >· 장점: 훈련 데이터 범위 밖의 샘플 예측O
  >· 단점: 비선형 데이터 정확도↓
  >```
  >
  >다중 선형 회귀(Multiple Linear Regression)
  >```
  >특성(2개 이상)과 타겟 사이의 관계를 가장 잘 나타내는 선형방정식(평면)으로 예측
  >예) y = axⁿ + bx^n-1 + ... + cx + d
  >
  >- 특성 공학: 기존의 특성을 사용하여 새로운 특성 추가(Feature Engineering)
  >  예) 특성a, 특성b → 특성a X 특성b, 특성a², 특성b²
  >
  >· 장점: 비선형 데이터 정확도↑
  >· 단점: 특성↑ ⇒ 과대적합
  >```
  >
  >릿지(L2)＆라쏘(L1)(Ridge＆Lasso)
  >```
  >규제를 추가한 선형 회귀
  >
  >최적화: 기울기(계수)↓
  >
  >- 규제 적용 기준
  >  릿지: 계수²
  >  라쏘: ㅣ계수ㅣ
  >
  >· 장점: 과대적합 완화
  >```
  >확률적 경사 하강법(Gradient Descent)
  >```
  >- 가설 함수: 직선
  >- 비용 함수: 평균제곱오차(Mean Squared Error, MSE)
  >```
  >랜덤 포레스트
  >```
  >부트스트랩 샘플과 무작위로 특성을 선택해서 여러 개의 결정 트리를 만들어서
  >각 결정 트리의 타겟별 확률의 평균값으로 예측
  >
  >부트스트랩 샘플O
  >무작위 특성 선택
  >```

+ ### 분류: 명목형 데이터
  샘플을 여러 개의 종류(타겟) 중 하나로 분류하는 문제
  ```angular2html
  - 이진 분류: 타겟 1개(음성: 0, 양성:1)
  - 다중 분류: 타겟 2개 이상
  
  예) 성별, 색깔, 혈액형, ...
  ```
  >K-최근접 이웃 분류(K-Nearest Neighbor Classifier)
  >```
  >데이터에서 거리가 가까운 K개 샘플들의 다수를 참고하여 분류
  >
  >· 단점
  >  - 항상 정해진 확률 출력(예: K=3 → 0/3, 1/3, 2/3, 3/3)
  >  - 대용량 데이터 부적합(계산 시간↑, 메모리↑)
  >```
  >
  >로지스틱 회귀(Logistic Regression)
  >```
  >데이터가 타겟에 속할 확률(0 ~ 1)을 기준으로 분류
  >예) 남자 60%, 여자 40% → 남자
  >    빨강 30%, 초록 30%, 파랑 40% → 파랑
  >
  >- z값(선형방정식) → 시그모이드 함수(이진 분류)＆소프트맥스(다중 분류) 함수 → 확률(0 ~ 1) 
  >
  >- 가설 함수: 시그모이드(이진 분류), 소프트맥스(다중 분류)
  >- 비용 함수: 로지스틱(이진 분류), 크로스 엔트로피(다중 분류)
  >```
  >
  >확률적 경사 하강법(Stochastic Gradient Descent)
  >```
  >훈련 데이터에서 한 개의 샘플을 비복원 추출하여 비용 함수를 최적화하는 알고리즘
  >
  >- 미니배치 경사 하강법: 여러 개의 샘플을 사용 ★
  >- 배치 경사 하강법: 전체 샘플을 사용
  >
  >- 에포크: 훈련 데이터를 한 번 모두 사용한 횟수
  >
  >- 비용 함수: 로지스틱(이진 분류), 크로스 엔트로피(다중 분류)
  >
  >· 장점: 메모리 소모량이 적어 대용량 데이터 학습 가능
  >```
  >---
  >
  >나이브 베이즈(Naive Bayes)
  >```
  >데이터가 어떤 분류에 속할 조건부 확률(0 ~ 1)값을 기준으로 분류
  >```
  >
  >서포트 벡터 머신(Support Vector Machine)
  >```
  >결정경계를 기준으로 분류
  >
  >최적화: 마진 최대
  >
  >- 마진 = ㅣ서포트 벡터 - 결정경계ㅣ
  >- 결정경계: 분류를 결정짓는 경계
  >- 서포트 벡터: 결정경계에 영향을 주는 데이터
  >```
  >
  >결정 트리(Decision tree)
  >```
  >데이터를 특성의 조건에 따라 분류
  >
  >최적화: 정보 이득 최대화 노드 분할
  >
  >- 노드(네모 상자): 훈련 데이터 정보(특성 조건, 불순도, 샘플 개수)
  >  ▶ 루트(시작) 노드: 맨 위 노드
  >  ▶ 리프(끝) 노드: 맨 아래 노드
  >
  >- 가지: 조건 결과(Yes / No)
  >
  >- 불순도: 최적의 질문을 찾기 위한 기준
  >  ▶ 지니 = 1 - (음성 타겟 비율² + 양성 타겟 비율²)
  >  ▶ 엔트로피 = - ( 음성 타겟 비율 X log₂(음성 타겟 비율) + 양성 타겟 비율 X log₂(양성 타겟 비율) )
  >
  >- 정보 이득: 부모(상위)와 자식(하위) 노드의 불순도 차이
  >  ▶ 부모 불순도
  >     - (왼쪽 노드 샘플 개수 / 부모 노드 샘플 개수) X 왼쪽 노드 불순도 
  >     - (오른쪽 노드 샘플 개수 / 부모 노드 샘플 개수) X 오른쪽 노드 불순도
  > 
  >- 특성 중요도: 특성이 불순도에 기여한 정도
  >
  >· 장점: 분류 과정을 시각적으로 이해하기 쉬움
  >· 단점: 과대적합↑
  >```
  >---
  >### 앙상블 ★
  >여러 개의 분류 모델을 병렬＆직렬로 결합해서 한 개의 최종 결과값을 구하는 방법
  >```
  >- 약한 분류 모델 → 병렬/직렬 → 강한 분류 모델
  >
  >· 장점: 결정 트리 단점 보완 
  >```
  >#### 보팅, 배깅, 부스팅
  > 
  >---
  >#### 보팅(Votting)
  >```
  >서로 다른 분류 모델 / 병렬
  >
  >- soft: 각 모델별 확률합의 평균을 구한 후 최대값을 선택
  >- hard: 각 모델의 결과값 중 가장 많은걸 선택(다수결의 원칙)
  >```
  >
  >#### 배깅(Bagging)
  >```
  >동일한 분류 모델 / 병렬
  >
  >- 부트스트랩 샘플: 훈련 데이터에서 복원 추출한 데이터
  >- OOB(Out Of Bag) 샘플: (전체 - 부트스트랩) 샘플
  >```
  >
  >+ 랜덤포레스트(Random Forest)
  >  ```
  >  부트스트랩 샘플과 무작위로 특성을 선택해서 여러 개의 결정 트리를 만들고
  >  각 결정 트리의 타겟별 확률을 평균한 후, 가장 높은 확률을 가진 타겟으로 분류
  >
  >  무작위 특성 선택
  >  부트스트랩 샘플O
  >  
  >  · 장점: 과대적합↓ ＆ 일반화 성능↑
  >  ```
  >+ 엑스트라 트리(Extra Trees)
  >  ```
  >  부트스트랩 샘플을 사용하지 않고 무작위로 특성을 선택해서 여러 개의 결정 트리를 만들고
  >  각 결정 트리의 타겟별 확률을 평균한 후, 가장 높은 확률을 가진 타겟으로 분류
  >
  >  무작위 특성 선택
  >  부트스트랩 샘플X
  >  무작위 노드 분할(최적화X)
  >  
  >  · 장점: 빠른 계산 속도(최적화X)
  >  · 단점: 많은 결정 트리 필요
  >  ```
  >
  >#### 부스팅(Boosting)
  >```
  >동일한 분류 모델 / 직렬(순차적)
  >
  >- AdaBoost: 가중치
  >```
  >
  >+ 그래디언트(Gradient Boosting)
  >  ```
  >  깊이가 얕은 결정 트리를 연속적으로 추가하여 이전 결정 트리의 오차를 보완하는 앙상블
  >  
  >  - 결정 트리 + 경사 하강법
  >
  >  · 장점: 과대적합↓ ＆ 일반화 성능↑
  >  · 단점: 훈련 속도 느림(직렬)
  >  ```
  >+ 히스토그램 기반 그래디언트(Histogram-based Gradient Boosting)
  >  ```angular2html
  >  훈련 데이터 -> 256개 구간
  >  
  >  · 장점: 과대적합↓ ＆ 그래디언트 성능 속도↑(노드 분할 속도 매우 빠름)
  >  ```
  >  >XGBoost: Gradientboost - 성능↑ ★
  >  >```
  >  > 
  >  >``` 
  >  >  
  >  >LightGBM: Gradientboost - 성능↑ 속도↑ 메모리↓ ★
  >  >```
  >  > 
  >  >``` 
     
## 비지도 학습
데이터(특성) → 모델 → 특징 찾기

+ ### 군집
    비슷한 샘플 그룹화
    ```
    클러스터: 군집 그룹
    클러스터 중심(= 센터로이드): 특성 평균값
  
    실루엣 계수: 군집 적합도(0~1)
    엘보우 계수: 오차제곱합(SSE)
    ```
    >K-평균(K-means, 비계층적)
    >```
    >클러스터 중심에서 거리가 가장 가까운 샘플들의 평균값으로 중심을 이동하면서 그룹화
    >
    >최적화: 엘보우 계수 변화 최대
    >
    >- 무작위로 K개의 클러스터 중심 설정
    >  → 클러스터 생성 → 클러스터 중심 이동 → 변화O
    >                ...
    >  → 클러스터 생성 → 클러스터 중심 이동(변화X) -> 종료
    >```
    >
    >계층적(hierarchy)
    >```
    >거리가 가까운 데이터들을 순차적으로 그룹화하여 군집화 방법에 따라 군집 수를 축소
    >
    >- 덴드로그램(Dendrogram): 군집화 과정을 보여주는 나무 형식 그림
    >
    >- 최단/최장연결: 서로 다른 군집에서 가장 가까운/먼 대상 거리 기준
    >- 평균/중심연결: 서로 다른 군집의 평균/중심 거리 기준
    >- 와드연결: 두 군집이 합쳐졌을 때 증가하는 오차 제곱합의 증가량 기준
    >
    >비계층적(k-평균) - 확인적
    >-> 계층적 군집분석보다 속도 빠름
    >-> 군집의 수를 알고 있는 경우 이용
    >-> k는 미리 정하는 군집 수
    >-> 계층적 군집화의 결과에 의거하여 군집 수 결정
    >-> 변수보다 관측대상 군집화에 많이 이용
    >->군집의 중심(Cluster Center)은 사용자가 정함
    >```
    >
+ ### 차원 축소
    특성 개수 축소(=특성 추출)
    ```
    · 장점: 데이터 크기↓ ＆ 모델 성능↑ ＆ 시각화(3 이하 축소)
    ```
    >주성분 분석(Principal Component Analysis, PCA)
    >```angular2html
    >데이터에서 주성분(가장 분산이 큰 방향 축)을 생성한 후,
    >데이터 손실을 최소화하여 주성분에 데이터를 이동시키는 방법
    >
    >- 분산: 데이터의 퍼짐 정도
    >- 설명된 분산: 분산 유지 정도(데이터 보존 정도)
    >
    >- 주성분 개수 ≤ 특성 개수
    >
    >예) 3차원 데이터 주성분 - PC1(73%), PC2(17%), PC3(10%)
    >    - 데이터 90(73+17)% 보존: 3차원 → 2차원 (PC1, PC2 - 주성분 개수: 2)
    >    - 데이터 73% 보존: 3차원 → 1차원 (PC1 - 주성분 개수: 1)
    >```
    >커널 PCA(Kernel Principal Component Analysis, KPCA)
    >```
    >
    >```

+ ### 이상치 탐지

+ ### 특이치 탐지

+ ### 연관 규칙 학습


## 강화 학습
