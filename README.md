># 머신러닝
>컴퓨터 학습 알고리즘(machine-learning)
>### 용어, 지도＆비지도＆강화 학습, 순서

## 용어
```
행
- 샘플

열
- 특성(feature) = 입력 = 독립변수 = 예측 변수
- 타겟(target) = 정답 = (클래스)레이블 = 종속변수 = 반응 변수

모델 파라미터: 모델 학습 변수(자동)
- 가중치: 중요도
- 편향: 추가 버프

하이퍼 파라미터: 모델 성능 조절 변수(수동)
- 비지도 학습: 
- 지도 학습: 교차 검증, 그리드 서치, 랜덤 서치 → 하이퍼 파라미터 최적화
  예) 회귀:  
  예) 분류: 

최적화: 오차(실제값-예측값)를 최소화하는 가중치＆편향

배치 학습: 모든 데이터를 한 번에 사용하여 학습하는 방법
점진적 학습: 데이터를 조금씩 사용하여 점진적으로 학습하는 방법
예) 확률적 경사 하강법
```

---
## 지도 학습
데이터(특성+타겟) → 모델 → 데이터 예측
```angular2html
과대적합
- 훈련 데이터 정확도 >> 테스트 데이터 정확도

규제: 훈련 데이터 과대적합 완화

과소적합
- 훈련 데이터 정확도 < 테스트 데이터 정확도
- 훈련,테스트 데이터 정확도↓↓

손실 함수: 훈련 데이터에서 샘플 한 개에 대한 손실
비용 함수: 훈련 데이터에서 모든 샘플에 대한 손실의 합
```
+ ### 회귀: 연속형 데이터
  임의의 어떤 숫자를 예측하는 문제
  ```
  가중치 = 기울기(계수) 
  편향 = 절편
  
  특성 - 예측 변수
  타겟 - 반응 변수
  
  가설 함수: x축-독립변수, y축-종속변수
  비용 함수: x축-가중치, y축-오차

  최적화: 비용 함수에서 가중치(미분=0)를 탐색하는 과정
  
  예) 경제 성장률, 배달 도착 시간, ...
  ```
  >K-최근접 이웃 회귀(K-Nearest Neighbor Regression)
  >```
  >데이터에서 거리가 가까운 K개 샘플들의 평균값으로 예측
  >
  >최적화
  >- 과대적합 → K 증가 → 모델 단순
  >- 과소적합 → K 감소 → 모델 복잡
  >
  >· 단점: 훈련 데이터 범위 밖의 샘플 예측X
  >```
  >
  >선형 회귀(Linear Regression)
  >```
  >특성(1개)과 타겟 사이의 관계를 가장 잘 나타내는 선형방정식(직선)으로 예측
  >예) y = ax + b
  >
  >- 가설 함수: 직선
  >- 비용 함수: 최소제곱법(Ordinary Least Squares, OLS)
  >
  >· 장점: 훈련 데이터 범위 밖의 샘플 예측O
  >· 단점: 비선형 데이터 정확도↓
  >```
  >
  >다중 선형 회귀(Multiple Linear Regression)
  >```
  >특성(2개 이상)과 타겟 사이의 관계를 가장 잘 나타내는 선형방정식(평면)으로 예측
  >예) y = axⁿ + bx^n-1 + ... + cx + d
  >
  >- 특성 공학: 기존의 특성을 사용하여 새로운 특성 추가(Feature Engineering)
  >  예) 특성a, 특성b → 특성a X 특성b, 특성a², 특성b²
  >
  >· 장점: 비선형 데이터 정확도↑
  >· 단점: 특성↑ ⇒ 과대적합
  >```
  >
  >릿지(L2)＆라쏘(L1)(Ridge＆Lasso)
  >```
  >규제를 추가한 선형 회귀
  >
  >최적화: 기울기(계수)↓
  >
  >- 규제 적용 기준
  >  릿지: 계수²
  >  라쏘: ㅣ계수ㅣ
  >
  >· 장점: 과대적합 완화
  >```
  >확률적 경사 하강법(Gradient Descent)
  >```
  >- 가설 함수: 직선
  >- 비용 함수: 평균제곱오차(Mean Squared Error, MSE)
  >```
  >랜덤 포레스트
  >```
  >부트스트랩 샘플과 무작위로 특성을 선택해서 여러 개의 결정 트리를 만들어서
  >각 결정 트리의 타겟별 확률의 평균값으로 예측
  >
  >부트스트랩 샘플O
  >무작위 특성 선택
  >```

+ ### 분류: 명목형 데이터
  샘플을 여러 개의 종류(타겟) 중 하나로 분류하는 문제
  ```angular2html
  - 이진 분류: 타겟 1개(음성: 0, 양성:1)
  - 다중 분류: 타겟 2개 이상
  
  예) 성별, 색깔, 혈액형, ...
  ```
  >K-최근접 이웃 분류(K-Nearest Neighbor Classifier)
  >```
  >데이터에서 거리가 가까운 K개 샘플들의 다수를 참고하여 분류
  >
  >· 단점
  >  - 항상 정해진 확률 출력(예: K=3 → 0/3, 1/3, 2/3, 3/3)
  >  - 대용량 데이터 부적합(계산 시간↑, 메모리↑)
  >```
  >
  >로지스틱 회귀(Logistic Regression)
  >```
  >데이터가 타겟에 속할 확률(0 ~ 1)을 기준으로 분류
  >예) 남자 60%, 여자 40% → 남자
  >    빨강 30%, 초록 30%, 파랑 40% → 파랑
  >
  >- z값(선형방정식) → 시그모이드 함수(이진 분류)＆소프트맥스(다중 분류) 함수 → 확률(0 ~ 1) 
  >
  >- 가설 함수: 시그모이드(이진 분류), 소프트맥스(다중 분류)
  >- 비용 함수: 로지스틱(이진 분류), 크로스 엔트로피(다중 분류)
  >```
  >
  >확률적 경사 하강법(Stochastic Gradient Descent)
  >```
  >훈련 데이터에서 한 개의 샘플을 비복원 추출하여 비용 함수를 최적화하는 알고리즘
  >
  >- 미니배치 경사 하강법: 여러 개의 샘플을 사용 ★
  >- 배치 경사 하강법: 전체 샘플을 사용
  >
  >- 에포크: 훈련 데이터를 한 번 모두 사용한 횟수
  >
  >- 비용 함수: 로지스틱(이진 분류), 크로스 엔트로피(다중 분류)
  >
  >· 장점: 메모리 소모량이 적어 대용량 데이터 학습 가능
  >```
  >---
  >
  >나이브 베이즈(Naive Bayes)
  >```
  >데이터가 어떤 분류에 속할 조건부 확률(0 ~ 1)값을 기준으로 분류
  >```
  >
  >서포트 벡터 머신(Support Vector Machine)
  >```
  >결정경계를 기준으로 분류
  >
  >최적화: 마진 최대
  >
  >- 마진 = ㅣ서포트 벡터 - 결정경계ㅣ
  >- 결정경계: 분류를 결정짓는 경계
  >- 서포트 벡터: 결정경계에 영향을 주는 데이터
  >```
  >
  >결정 트리(Decision tree)
  >```
  >데이터를 특성의 조건에 따라 분류
  >
  >최적화: 정보 이득 최대화 노드 분할
  >
  >- 노드: 훈련 데이터 특성의 조건(네모 상자)
  >  루트 노드: 시작 노드        리프 노드: 끝 노드
  >
  >- 가지: 조건 결과(Yes / No)
  >
  >- 불순도: 최적의 질문을 찾기 위한 기준
  >  지니 = 1 - (음성 타겟 비율² + 양성 타겟 비율²)
  >  엔트로피 = - ( 음성 타겟 비율 X log₂(음성 타겟 비율) + 양성 타겟 비율 X log₂(양성 타겟 비율) )
  >
  >- 정보 이득: 부모(상위)와 자식(하위) 노드의 불순도 차이
  >  ▶ 부모 불순도
  >     - (왼쪽 노드 샘플 개수 / 부모 노드 샘플 개수) X 왼쪽 노드 불순도 
  >     - (오른쪽 노드 샘플 개수 / 부모 노드 샘플 개수) X 오른쪽 노드 불순도
  > 
  >- 특성 중요도: 특성이 불순도에 기여한 정도
  >
  >· 장점: 분류 과정을 시각적으로 이해하기 쉬움
  >```
  >---
  >### 앙상블 ★
  >여러 개의 분류 모델을 병렬＆직렬로 결합해서 한 개의 최종 결과값을 구하는 방법
  >```
  >- 약한 분류 모델 → 병렬/직렬 → 강한 분류 모델
  >
  >· 장점: 결정 트리 단점 보완 
  >```
  >#### 보팅, 배깅, 부스팅
  > 
  >---
  >#### 보팅(Votting)
  >```
  >서로 다른 분류 모델 / 병렬
  >
  >- soft: 각 모델별 확률합의 평균을 구한 후 최대값을 선택
  >- hard: 각 모델의 결과값 중 가장 많은걸 선택(다수결의 원칙)
  >```
  >
  >#### 배깅(Bagging)
  >```
  >동일한 분류 모델 / 병렬
  >
  >- 부트스트랩 샘플: 훈련 데이터에서 복원 추출한 데이터
  >- OOB(Out Of Bag) 샘플: (전체 - 부트스트랩) 샘플
  >```
  >
  >+ 랜덤포레스트(Random Forest)
  >  ```
  >  부트스트랩 샘플과 무작위로 특성을 선택해서 여러 개의 결정 트리를 만들고
  >  각 결정 트리의 타겟별 확률을 평균한 후, 가장 높은 확률을 가진 타겟으로 분류
  >
  >  무작위 특성 선택
  >  부트스트랩 샘플O
  >  
  >  · 장점: 과대적합↓ ＆ 일반화 성능↑
  >  ```
  >+ 엑스트라 트리(Extra Trees)
  >  ```
  >  부트스트랩 샘플을 사용하지 않고 무작위로 특성을 선택해서 여러 개의 결정 트리를 만들고
  >  각 결정 트리의 타겟별 확률을 평균한 후, 가장 높은 확률을 가진 타겟으로 분류
  >
  >  무작위 특성 선택
  >  부트스트랩 샘플X
  >  무작위 노드 분할(최적화X)
  >  
  >  · 장점: 빠른 계산 속도(최적화X)
  >  · 단점: 많은 결정 트리 필요
  >  ```
  >
  >#### 부스팅(Boosting)
  >```
  >동일한 분류 모델 / 직렬(순차적)
  >
  >- AdaBoost: 가중치
  >```
  >
  >+ 그래디언트(Gradient Boosting)
  >  ```
  >  깊이가 얕은 결정 트리를 연속적으로 추가하여 이전 결정 트리의 오차를 보완하는 앙상블
  >  
  >  - 결정 트리 + 경사 하강법
  >
  >  · 장점: 과대적합↓ ＆ 일반화 성능↑
  >  · 단점: 훈련 속도 느림(직렬)
  >  ```
  >+ 히스토그램 기반 그래디언트(Histogram-based Gradient Boosting)
  >  ```angular2html
  >  훈련 데이터 -> 256개 구간
  >  
  >  · 장점: 과대적합↓ ＆ 그래디언트 성능 속도↑(노드 분할 속도 매우 빠름)
  >  ```
  >  >XGBoost: Gradientboost - 성능↑ ★
  >  >```
  >  > 
  >  >``` 
  >  >  
  >  >LightGBM: Gradientboost - 성능↑ 속도↑ 메모리↓ ★
  >  >```
  >  > 
  >  >``` 
     
## 비지도 학습
데이터(특성) → 모델 → 특징 찾기

+ ### 군집
    비슷한 샘플 그룹화
    ```
    클러스터: 군집 그룹
    클러스터 중심(= 센터로이드): 특성 평균값
  
    실루엣 계수: 군집 적합도(0~1)
    엘보우 계수: 오차제곱합(SSE)
    ```
    >K-평균(K-means, 비계층적)
    >```
    >클러스터 중심에서 거리가 가장 가까운 샘플들의 평균값으로 중심을 이동하면서 그룹화
    >
    >최적화: 엘보우 계수 변화 최대
    >
    >- 무작위로 K개의 클러스터 중심 설정
    >  → 클러스터 생성 → 클러스터 중심 이동 → 변화O
    >                ...
    >  → 클러스터 생성 → 클러스터 중심 이동(변화X) -> 종료
    >```
    >
    >계층적(hierarchy)
    >```
    >거리가 가까운 데이터들을 순차적으로 그룹화하여 군집화 방법에 따라 군집 수를 축소
    >
    >- 덴드로그램(Dendrogram): 군집화 과정을 보여주는 나무 형식 그림
    >
    >- 최단/최장연결: 서로 다른 군집에서 가장 가까운/먼 대상 거리 기준
    >- 평균/중심연결: 서로 다른 군집의 평균/중심 거리 기준
    >- 와드연결: 두 군집이 합쳐졌을 때 증가하는 오차 제곱합의 증가량 기준
    >
    >비계층적(k-평균) - 확인적
    >-> 계층적 군집분석보다 속도 빠름
    >-> 군집의 수를 알고 있는 경우 이용
    >-> k는 미리 정하는 군집 수
    >-> 계층적 군집화의 결과에 의거하여 군집 수 결정
    >-> 변수보다 관측대상 군집화에 많이 이용
    >->군집의 중심(Cluster Center)은 사용자가 정함
    >```
    >
+ ### 차원 축소
    특성 개수 축소(=특성 추출)
    ```
    · 장점: 데이터 크기↓ ＆ 모델 성능↑ ＆ 시각화(3 이하 축소)
    ```
    >주성분 분석(Principal Component Analysis, PCA)
    >```angular2html
    >데이터에서 주성분(가장 분산이 큰 방향 축)을 생성한 후,
    >데이터 손실을 최소화하여 주성분에 데이터를 이동시키는 방법
    >
    >- 분산: 데이터의 퍼짐 정도
    >- 설명된 분산: 분산 유지 정도(데이터 보존 정도)
    >
    >- 주성분 개수 ≤ 특성 개수
    >
    >예) 3차원 데이터 주성분 - PC1(73%), PC2(17%), PC3(10%)
    >    - 데이터 90(73+17)% 보존: 3차원 → 2차원 (PC1, PC2 - 주성분 개수: 2)
    >    - 데이터 73% 보존: 3차원 → 1차원 (PC1 - 주성분 개수: 1)
    >```
    >커널 PCA(Kernel Principal Component Analysis, KPCA)
    >```
    >
    >```

## 강화 학습

---

## 순서
데이터 전처리 → 모델 생성＆학습 → 모델 평가 → 데이터 예측

+ ### 데이터 전처리1
    ```
    결측치
    속성 상관관계 시각화??
    라벨 인코딩
    필요없는 속성 삭제
    ```

+ ### 데이터 전처리2
    ```
    특성(+타겟) 선택
    데이터 분할(지도 학습)
    데이터 스케일링
    차원 축소
    ```

+ ### 모델 셍성＆학습
    ```angular2html
    모델(회귀, 분류, 군집) 선택
    ```

+ ### 모델 성능 조절
    ```angular2html
    교차 검증
    그리드 서치＆랜덤 서치(교차 검증 + 하이퍼파라미터 최적화)
    ```

+ ### 모델 평가
    ### 이진 분류
    >오차 행렬
    >```
    >예측값이 실제값을 얼마나 정확히 예측했는지 보여주는 행렬
    >
    >　　　　　　　　　　　　　예측 Negative　　　　　　　　예측 Positive
    >──────────────┼───────────────────────────────────────────────────
    >실제 Negative　│　　 정답(True Nagative)　　　 오답(False Positive)
    >실제 Positive　│　　오답(False Negative)　　　　정답(True Positive)
    >```
    >성능 지표
    >```
    >정확도: 정답 / 전체
    >(TN + TP) / TN + TF + FN + TP
    >
    >정밀도: 참이라고 예측할 때, 실제 참인 비율 ★
    >TP / (FP + TP)
    >
    >재현율: 실제 참일 때, 참이라고 예측한 비율 ★
    >TP / (FN + TP)
    >
    >특이도: 실제 거짓일 때, 거짓이라고 예측한 비율
    >```
    >
    >AUC
    >```
    >ROC 곡선 면적(Area Under Curve)
    >   범위: 0 ~ 1
    >   AUC↑ → 성능↑
    >
    >- ROC 곡선: 기준값에 따라 분류 모델 성능 변화를 나타내는 곡선(Receiver Operation Characteristic Curve)
    >- x축: FPR(1-특이도)
    >- y축: 재현율(recall)
    >- 기준값(threshold)
    >```
    >
    ### 회귀

+ ### 데이터 예측

