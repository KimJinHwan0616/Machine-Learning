># 분류
>### 모델 - 생성＆학습, 평가, 성능 조절

## 모델 생성＆학습
훈련 데이터 → 모델
```
(분류)변수 = 모델                               # 모델 생성

# ★모델 성능 조절★

# (분류)변수.fit( xtrain, ytrain )              # 모델 학습
(분류)변수.fit( xtrain_scaled, ytrain )         # 모델 학습(스케일링)

(분류)변수.classes_       # 타겟 목록

# (분류)변수.score( xtrain, ytrain )            # 훈련 데이터 정확도
# (분류)변수.score( xtest, ytest )              # 테스트 데이터 정확도

# (분류)변수.score( xtrain_scaled, ytrain )     # 훈련 데이터 정확도(스케일링)
# (분류)변수.score( xtest_scaled, ytest )       # 테스트 데이터 정확도(스케일링)
```
+ ### 모델
  >K-최근접 이웃 - knc
  >``` 
  >from sklearn.neighbors import KNeighborsClassifier
  >
  >KNeighborsClassifier(
  >    n_neighbors = 5    # 하이퍼 파라미터(최근접 이웃 수) ★
  >    , p = 2            # 거리 기준(1: 맨해튼, 2: 유클리디안)
  >    , n_jobs = 1       # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
  >    )
  >
  ># knc.kneighbors([[값]])    # ( 최근접 이웃까지의 거리, 최근접 이웃 인덱스 )
  >```
  >로지스틱 회귀 - lr
  >```
  >from sklearn.linear_model import LogisticRegression
  >
  >LogisticRegression(
  >    C = 1                   # 하이퍼 파라미터(규제 강도)
  >    , max_iter = 100        # 반복 횟수   
  >)
  >
  ># lr.coef_         # 기울기
  ># lr.intercept_    # 절편
  >```
  >
  >확률적 경사 하강법 - sgdc
  >```
  >from sklearn.linear_model import SGDClassifier
  >  
  >SGDClassifier(
  >    loss = 'log'         # 손실 함수(로그)
  >    , max_iter = 1000    # 하이퍼 파라미터(에포크 횟수)
  >    , tol = None         # 에포크(반복) 횟수 멈춤 조건
  >    )
  >  
  ># sgdclf.n_iter_    # 에포크(반복) 횟수
  >```
  >---
  >
  >나이브 베이즈 - mnb
  >```
  >from sklearn.naive_bayes import MultinomialNB
  >
  >BernoulliNB()
  >```
  >서포트 벡터 머신 - svc ㅇ
  >```
  >from sklearn.svm import SVC
  >
  >SVC(
  >    kernel='linear/rbf'               # 2차원/3차원↑
  >    , C = 정확도                      # 하이퍼 파라미터
  >    , gamma = 결정경계 곡률(3차원↑)    # 하이퍼 파라미터(곡률↑ → 결정경계 범위↓) 
  >    )    
  >```
  >
  >결정 트리 - dtc
  >```
  >from sklearn.tree import DecisionTreeClassifier
  >
  >DecisionTreeClassifier(
  >    criterion = 'gini/entropy'    # 불순도(지니/엔트로피)
  >    , max_depth = 숫자            # 하이퍼 파라미터(결정 트리 깊이)
  >    , min_samples_split = 2       # 노드 분할 최소 샘플 개수
  >    , max_features = None         # 노드 분할 최대 특성 개수(숫자)
  >    )
  >
  ># dtc.feature_importances_    # 특성 중요도
  >```
  >---
  >### 앙상블
  >보팅 - vc
  >```
  >from sklearn.ensemble import VotingClassifier
  >
  >voclf = VotingClassifier(
  >  voting='soft/hard'       # 투표 방식
  >  estimators=[ ('(분류)변수1', (분류)변수1), ..., ('(분류)변수n', (분류)변수n) ]
  >  )
  >```
  >배깅 - bc
  >```
  >from sklearn.ensemble import BaggingClassifier
  >
  >BaggingClassifier(
  >    DecisionTreeClassifier()
  >    , n_estimators = 모델 개수
  >    , bootstrap=True = True    # 부트스트랩
  >    , n_jobs = 1               # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
  >    , oob_score=True           # oob 정확도
  >    )
  >```
  >+ 랜덤포레스트 - rfc
  >  ```
  >  from sklearn.ensemble import RandomForestClassifier
  >
  >  RandomForestClassifier(
  >      criterion = 'gini/entropy'    # 불순도
  >      , n_estimators = 100          # 결정 트리 개수
  >      , max_depth = 숫자            # 하이퍼 파라미터(결정 트리 깊이)
  >      , min_samples_split = 2       # 노드 분할 최소 샘플 개수
  >      , max_features = auto         # 노드 분할 특성 개수(숫자)
  >      , n_jobs = -1                 # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
  >      , oob_score = True            # oob(검증 데이터) 정확도 추가
  >      )
  >  
  >  # rfc.feature_importances_    # 특성 중요도
  >  # rfc.oob_score_    # oob(검증 데이터) 정확도
  >  ```
  >  
  >+ 엑스트라 트리 - etc
  >  ```
  >  from sklearn.ensemble import ExtraTreesClassifier
  >
  >  ExtraTreesClassifier(
  >      criterion = 'gini/entropy'    # 불순도
  >      , n_estimators = 100          # 결정 트리 개수
  >      , max_depth = 숫자            # 하이퍼 파라미터(결정 트리 깊이)
  >      , min_samples_split = 2       # 노드 분할 최소 샘플 개수
  >      , max_features = auto         # 노드 분할 특성 개수(숫자)
  >      , n_jobs = -1                 # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
  >      , oob_score = True            # oob(검증 데이터) 정확도 추가
  >      )
  >  
  >  # etc.feature_importances_    # 특성 중요도
  >  # etc.oob_score_    # oob(검증 데이터) 정확도
  >  ```
  >#### 부스팅
  >AdaBoost - abc
  >```
  >from sklearn.ensemble import AdaBoostClassifier
  >
  >AdaBoostClassifier(
  >    DecisionTreeClassifier()            # 결정 트리 모델
  >    , n_estimators = 모델 개수
  >    , learning_rate = 학습률(범위: 0~1)
  >    , algorithm='SAMME.R/SAMME'         # soft/hard
  >    )
  >```
  >그래디언트 - gbc
  >```
  >from sklearn.ensemble import GradientBoostingClassifier
  >
  >GradientBoostingClassifier(
  >    loss = 'deviance'       # 손실 함수
  >    , learning_rate = 0.1   # 학습률
  >    , max_depth = 3         # 하이퍼 파라미터(추가 결정 트리 깊이)              
  >    , n_estimators = 100    # 결정 트리 개수
  >    , subsample = 1         # 경사 하강법 샘플 비율
  >    )
  >
  ># gbc.feature_importances_    # 특성 중요도
  >```
  >
  >히스토그램 기반 그레디언트 - gbc
  >```
  >from sklearn.ensemble import HistGradientBoostingClassifier
  >
  >GradientBoostingClassifier(
  >    max_iter = 100          # 하이퍼 파라미터(결정 트리 개수)
  >    , learning_rate = 0.1   # 학습률
  >    , max_bins = 255        # 입력 데이터 나눌 구간
  >    )
  >
  ># 특성 중요도 함수
  >result = permutation_importance(
  >    hgbc
  >    , xtrain, ytrain
  >    , n_repeats = 5    # 특성을 랜덤하게 섞을 횟수
  >    , n_jobs = 1       # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
  >    )
  >
  >result.importances_mean      # 특성 중요도
  >
  ># result.importances_mean    # 평균
  ># result.importances_std     # 표준 편차
  >```
  >+ XGBoost - xgbc
  >  ```
  >  from xgboost import XGBClassifier
  >
  >  GradientBoostingClassifier(
  >      max_depth = 숫자               # 결정 트리 최대깊이
  >      tree_method = 'hist'
  >      )
  >  ```
  >+ LightGBM - lgbmc
  >  ```
  >  from lightgbm import LGBMClassifier
  >  
  >  LGBMClassifier()
  >  ```
  
---
## 모델 평가
테스트 데이터 → 모델 → 평가

```
from sklearn.metrics import confusion_matrix    # 오차 행렬
from sklearn.metrics import 성능지표
from sklearn.metrics import roc_auc_score       # AUC

# 테스트 데이터 → 모델
# pred = (분류)변수.predict(xtest)         # 타겟 예측값
pred = (분류)변수.predict(xtest_scaled)    # 타겟 예측값(스케일링)

# (분류)변수.predict_proba(xtest[: 1])       # 타겟(양성) 예측 확률값
# (분류)변수.predict_proba(xtest[: 0])       # 타겟(음성) 예측 확률값

# 평가
def eval(ytest, pred=None):
    confusion = confusion_matrix(ytest, pred)    # 오차 행렬
    accuracy = accuracy_score(ytest, pred)       # 정확도
    precision = precision_score(ytest, pred)     # 정밀도
    recall = recall_score(ytest, pred)           # 재현율
    f1 = f1_score(ytest,pred)                    # F1
    roc_auc = roc_auc_score(ytest, pred)         # AUC

    print('오차행렬')
    print(confusion)
    print()
    print(f'정확도: {accuracy}, 정밀도: {precision}, 재현율: {recall}')
    print(f'F1: {f1}, AUC: {roc_auc}')

eval(ytest, pred)   # 성능 지표
```

+ ### ROC 곡선
    ```
    from sklearn.metrics import roc_curve
    
    # 테스트 데이터 → 예측
    # pred_proba = (분류)변수.predict_proba(Xtest[:, 0])    # 확률(1열) 예측값
    pred_proba = (분류)변수.predict_proba(Xtest[:, 1])      # 확률(2열) 예측값

    fpr, sensitivity, threshold = roc_curve(ytest, pred_proba)  # ROC 곡선
    
    roc = pd.DataFrame()
    roc['fpr'] = fpr
    roc['sensitivity'] = sensitivity
    roc['threshold'] = threshold
    
    # 시각화
    import matplotlib.pyplot as plt
    
    plt.plot(fpr, sensitivity)   
    plt.plot([0,1],[0,1], 'r--')   
    plt.show()
    ```

---
## 모델 성능 조절
훈련+검증 데이터 → 모델 학습 → 모델 평가 → 하이퍼 파라미터 최적화
```
검증 데이터(모의고사): 하이퍼 파라미터 최적화를 위해 모델 성능을 평가할 때,
테스트 데이터를 계속 사용하지 않기 위해 훈련 데이터에서 분할한 데이터

그리드, 랜덤 서치: 교차 검증을 통해 하이퍼 파라미터 최적화한 값에서 훈련 데이터로 모델 학습
```
+ ### K-겹 교차 검증
    중복을 허용하지 않고 훈련 데이터를 임의의 K개로 분할(K-fold Cross Validation)
    ```
    훈련 데이터: K-1개 → 검증 데이터: 1개 → 모델 학습 → 모델 평가
    → K번 반복 ★
    → 모델 평가(평균)
    ```

    ```
    from sklearn.model_selection import cross_validate, StratifiedKFold
    
    # skf = StratifiedKFold(n_splits = k, shuffle = True)

    scores = cross_validate(
        (분류)변수
        , xtrain, ytrain               # 특성, 타겟
    #   , xtrain_scaled, ytrain        # 특성(스케일링), 타겟
        , scoring = 'accuracy'         # 평가 지표(정확도)
        , return_train_score = True    # 훈련 데이터 정확도 추가
        , cv = 5                       # 교차 검증 횟수(K번)
    #   , cv = skf                     # 훈련 데이터 섞기
        , n_jobs = 1                   # 사용할 CPU 개수(-1: 모두, 1: 한 개)
        )
    
    scores    # 모델 학습 시간, 모델 검증 시간, 훈련 데이터 정확도, 검증 데이터 정확도
    
    # np.mean(scores['train_score'])    # 훈련 데이터 정확도(평균)
    # np.mean(scores['test_score'])     # 검증 데이터 정확도(평균)
    ```

+ ### 그리드 서치
    K-겹 교차 검증 + 하이퍼 파라미터 최적화 ★<br>
    매개변수 범위 ``직접`` 설정
    ```
    from sklearn.model_selection import GridSearchCV

    params = {
        '하이퍼 파라미터1': np.arange(시작값, 종료값+1, 증가값),
                   ...
        '하이퍼 파라미터n': np.arange(시작값, 종료값+1, 증가값),
        }
    
    gscv = GridSearchCV(
        (분류)변수       # 모델
        , params        # 하이퍼 파라미터 목록
        , cv = 5        # 교차 검증 횟수(K번)
        , scoring = 'accuracy'         # 평가 지표(정확도)
        , return_train_score = True    # 훈련 데이터 정확도 추가
        , n_jobs = 1    # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
        )
    
    # gscv.fit(xtrain, ytrain)
    gscv.fit(xtrain_scaled, ytrain)
  
    (분류)변수 = gscv.best_estimator_    # 하이퍼 파라미터 최적화 값 저장
    gscv.best_params_    # 하이퍼 파라미터 최적화 값
    
    # gscv.cv_results_['mean_train_score']    # 하이퍼 파라미터 훈련 데이터 정확도
    # gscv.cv_results_['mean_test_score']     # 하이퍼 파라미터 검증 데이터 정확도
  
    # np.max(gscv.cv_results_['mean_train_score'])    # 하이퍼 파라미터 최적화 훈련 데이터 정확도
    # np.max(gscv.cv_results_['mean_test_score'])     # 하이퍼 파라미터 최적화 검증 데이터 정확도
    ``` 

+ ### 랜덤 서치
    K-겹 교차 검증 + 하이퍼 파라미터 최적화 ★<br>
    매개변수 범위 ``랜덤`` 설정
    ```
    from scipy.stats import uniform, randint
    from sklearn.model_selection import RandomizedSearchCV

    params = {
        '하이퍼 파라미터1': uniform/randint(시작값, 종료값),    # 난수(실수/정수) 생성
                   ...
        '하이퍼 파라미터n': uniform/randint(시작값, 종료값),    # 난수(실수/정수) 생성
        }
    
    rscv = RandomizedSearchCV(
        (분류)변수         # 모델
        , params          # 하이퍼 파라미터 목록
        , n_iter = 10     # 난수 생성 횟수
        , cv = 5          # 교차 검증 횟수(K번)
        , n_jobs = 1      # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
        , scoring = 'accuracy'         # 평가 지표(정확도)
        , return_train_score = True    # 훈련 데이터 정확도 추가
        )
    
    # rscv.fit(xtrain, ytrain)
    rscv.fit(xtrain_scaled, ytrain)
  
    (분류)변수 = rscv.best_estimator_    # 하이퍼 파라미터 최적화 값 저장
    rscv.best_params_  # 하이퍼 파라미터 최적화 값
    
    # rscv.cv_results_['mean_train_score']    # 하이퍼 파라미터 훈련 데이터 정확도
    # rscv.cv_results_['mean_test_score']     # 하이퍼 파라미터 검증 데이터 정확도
  
    # np.max(rscv.cv_results_['mean_train_score'])    # 하이퍼 파라미터 최적화 훈련 데이터 정확도
    # np.max(rscv.cv_results_['mean_test_score'])     # 하이퍼 파라미터 최적화 검증 데이터 정확도
    ```

---
>### 하이퍼파라미터 최적화
>확률적 경사 하강법 - max_iter
>```
>train_score = []
>test_score = []
>
>classes = np.unique( ytrain )
>
>for _ in range(0, 300):
>    sgdc.partial_fit( xtrain_scaled, ytrain, classes=classes )
>    train_score.append( sgdc.score( xtrain_scaled, ytrain ) )
>    test_score.append( sgdc.score( xtest_scaled, ytest ) )
>
># 시각화
>plt.plot(train_score)
>plt.plot(test_score)
>plt.xlabel('epoch')
>plt.ylabel('accuracy')
>plt.show()
>```
>
>서포트 벡터 머신 - C, gamma
>```
>
>```
>
>K-최근접 이웃 - n_neighbors
>```
>scores = []
>for k in range(1, 10+1):
>    knclf = KNeighborsClassifier(n_neighbors=k)
>#   score = cross_val_score( (분류)변수, xtrain, ytrain, cv=10, scoring='accuracy' )
>    score = cross_val_score( (분류)변수, xtrain_scaled, ytrain, cv=10, scoring='accuracy' )    # 스케일링
>    scores.append(np.mean(score))
>
># 시각화
>plt.plot(range(1, 10+1), scores, 'ro--')
>plt.show()
>```
>
>랜덤 포레스트 - 특성 중요도
>```
>코드 수정
>importance = rfclf.feature_importances_     # 특성 중요도
>
>descending = np.argsort(importance)[::-1]   # 내림차순
>names = [ (테이블)변수.feature_names[i] for i in descending ]
>
># 시각화 - 막대그래프
>plt.bar(range(특성 개수), importance[descending])
>plt.xticks(range(특성 개수), names)
>plt.show()
>```

