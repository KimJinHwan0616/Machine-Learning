># 회귀
>### 모델 - 생성＆학습, 평가, 성능 조절

## 모델 생성＆학습
훈련 데이터 → 모델
```
(회귀)변수 = 모델     # 모델 생성

# ★모델 성능 조절★

(회귀)변수.fit( xtrain, ytrain )    # 모델 학습

# (회귀)변수.score( xtrain, ytrain )    # 훈련 데이터 결정계수(≒정확도)
# (회귀)변수.score( xtest, ytest )      # 테스트 데이터 결정계수(≒정확도)
```
+ 모델
  >K-최근접 이웃 회귀 - knr
  >```
  >from sklearn.neighbors import KNeighborsRegressor
  >
  >KNeighborsRegressor(
  >    n_neighbors = 5    # 하이퍼 파라미터(최근접 이웃 수) ★
  >    )
  >
  ># knr.kneighbors([[값]])    # ( 최근접 이웃까지의 거리, 최근접 이웃 인덱스 )
  >```
  >
  >선형 회귀 - lr
  >```
  >from sklearn.linear_model import LinearRegression
  >
  >LinearRegression()
  >
  ># lr.coef_          # 기울기(계수)
  ># lr.intercept_     # 절편
  >```
  >
  >다중 선형 회귀 - lr
  >```
  >from sklearn.preprocessing import PolynomialFeatures
  >
  >pf = PolynomialFeatures(
  >    degree = 선형방정식 차수
  >    , interaction_only = True    # 거듭제곱항(특성²) 특성 포함
  >    , include_bias = False       # 절편(1) 특성 제외
  >    )
  >
  >xtrain = pf.fit_transform(xtrain)
  >xtest = pf.transform(xtest)
  >
  ># pf.get_feature_names_out()            # 특성 조합
  ># print( xtrain.shape, xtest.shape )    # (샘플 개수, 특성 개수)
  >
  >LinearRegression()
  >
  ># lr.coef_          # 기울기  
  ># lr.intercept_     # 절편
  >```
  >
  >릿지＆라쏘 - R＆L
  >```
  >from sklearn.linear_model import Ridge/Lasso
  >
  ># 특성 공학
  >pf = PolynomialFeatures(
  >    degree = 선형방정식 차수
  >    , interaction_only = True    # 거듭제곱항(특성²) 특성 포함
  >    , include_bias = False       # 절편(1) 특성 제외
  >    )
  >
  >xtrain = pf.fit_transform(xtrain)
  >xtest = pf.transform(xtest)
  >
  ># pf.get_feature_names_out()            # 특성 조합
  ># print( xtrain.shape, xtest.shape )    # (샘플 개수, 특성 개수)
  >
  ># 표준화
  >ss = StandardScaler()
  >
  >xtrain_scaled = ss.fit_transform(xtrain)    
  >xtest_scaled = ss.transform(xtest)          
  >
  ># 모델 학습
  >Ridge/Lasso(
  >    alpha = 1    # 하이퍼 파라미터(규제 강도)
  >    )
  >
  ># np.sum(lasso.coef_ == 0)    # 라쏘 모델: 계수=0 개수
  >```
  >확률적 경사 하강법 - sgdr
  >```
  >from sklearn.linear_model import SGDRegressor
  >  
  >SGDRegressor(
  >    loss = 'squared_loss'    # 손실 함수(평균제곱오차)
  >    , max_iter = 1000        # 하이퍼 파라미터(에포크 횟수)
  >    , tol = None             # 에포크(반복) 횟수 멈춤 조건
  >    )
  >```
  >랜덤 포레스트
  >```
  >from sklearn.ensemble import RandomForestRegressor
  >```
  >
  >엑스트라 트리
  >```
  >from sklearn.ensemble import ExtraTreesRegressor
  >```
  >
  >그래디언트 부스팅
  >```
  >from sklearn.ensemble import GradientBoostingRegressor
  >```

---
## 모델 평가
테스트 데이터 → 모델 → 평가
```
# 테스트 데이터 → 모델
pred = (회귀)변수.predict( np.array(xtest.reshape(-1,1)) )         # 이산 예측값

# 평가
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(ytest, pred)       # 평균 제곱 오차
rmse = np.sqrt(mse)                          # 루트 평균 제곱 오차
r2 = r2_score(ytest, pred)                  # 결정계수(≒정확도)

# (회귀)변수.predict( [ [값??] ] )            # 타겟 예측
# (회귀)변수.predict_proba( [[값??]] )        # 타겟 예측 확률값
```

---
## 모델 성능 조절
훈련+검증 데이터 → 모델 학습 → 모델 평가 → 하이퍼 파라미터 최적화
```
검증 데이터(모의고사): 하이퍼 파라미터 최적화를 위해 모델 성능을 평가할 때,
테스트 데이터를 계속 사용하지 않기 위해 훈련 데이터에서 분할한 데이터

그리드, 랜덤 서치: 교차 검증을 통해 하이퍼 파라미터 최적화한 값에서 훈련 데이터로 모델 학습
```
+ ### K-겹 교차 검증
    중복을 허용하지 않고 훈련 데이터를 임의의 K개로 분할(K-fold Cross Validation)
    ```
    훈련 데이터: K-1개 → 검증 데이터: 1개 → 모델 학습
    → K번 반복 
    → 평가(평균)
    ```

    ```
    from sklearn.model_selection import cross_validate, KFold

    # kf = KFold(n_splits = k, shuffle=True)

    scores = cross_validate(
        (회귀)변수
        , xtrain, ytrain               # 특성, 타겟
    #   , xtrain_scaled, ytrain        # 특성(스케일링), 타겟
        , scoring = 'r2'               # 평가 지표(결정계수)
        , return_train_score = True    # 훈련 데이터 결정계수 추가
        , cv = 5                       # 교차 검증 횟수(K번)
    #   , cv = kf                      # 훈련 데이터 섞기
        , n_jobs = 1                   # 사용할 CPU 개수(-1: 모두, 1: 한 개)
        )
    
    scores    # 모델 학습 시간, 모델 검증 시간, 각각 훈련, 검증 데이터 결정계수
    
    # np.mean(scores['train_score'])    # 훈련 데이터 결정계수(평균)
    # np.mean(scores['test_score'])     # 검증 데이터 결정계수(평균)
    ```
  
+ ### 그리드 서치
    K-겹 교차 검증 + 하이퍼 파라미터 최적화 ★<br>
    매개변수 범위 ``직접`` 설정
    ```
    from sklearn.model_selection import GridSearchCV

    params = {
        '하이퍼 파라미터1': np.arange(시작값, 종료값+1, 증가값),
                   ...
        '하이퍼 파라미터n': np.arange(시작값, 종료값+1, 증가값),
        }
    
    gscv = GridSearchCV(
        (회귀)변수       # 모델
        , params        # 하이퍼 파라미터 목록
        , cv = 5        # 교차 검증 횟수(K번)
        , scoring = 'accuracy'         # 평가 지표(정확도)
        , return_train_score = True    # 훈련 데이터 정확도 추가
        , n_jobs = 1    # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
        )
    
    # gscv.fit(xtrain, ytrain)
    gscv.fit(xtrain_scaled, ytrain)
  
    (회귀)변수 = gscv.best_estimator_    # 하이퍼 파라미터 최적화 값 저장
    gscv.best_params_    # 하이퍼 파라미터 최적화 값
    
    # gscv.cv_results_['mean_train_score']    # 하이퍼 파라미터 훈련 데이터 정확도
    # gscv.cv_results_['mean_test_score']     # 하이퍼 파라미터 검증 데이터 정확도
  
    # np.max(gscv.cv_results_['mean_train_score'])    # 하이퍼 파라미터 최적화 훈련 데이터 정확도
    # np.max(gscv.cv_results_['mean_test_score'])     # 하이퍼 파라미터 최적화 검증 데이터 정확도
    ``` 

+ ### 랜덤 서치
    K-겹 교차 검증 + 하이퍼 파라미터 최적화 ★<br>
    매개변수 범위 ``랜덤`` 설정
    ```
    from scipy.stats import uniform, randint
    from sklearn.model_selection import RandomizedSearchCV

    params = {
        '하이퍼 파라미터1': uniform/randint(시작값, 종료값),    # 난수(실수/정수) 생성
                   ...
        '하이퍼 파라미터n': uniform/randint(시작값, 종료값),    # 난수(실수/정수) 생성
        }
    
    rscv = RandomizedSearchCV(
        (회귀)변수         # 모델
        , params          # 하이퍼 파라미터 목록
        , n_iter = 10     # 난수 생성 횟수
        , cv = 5          # 교차 검증 횟수(K번)
        , n_jobs = 1      # 사용할 CPU 코어 개수(-1: 모두, 1: 한 개)
        , scoring = 'accuracy'         # 평가 지표(정확도)
        , return_train_score = True    # 훈련 데이터 정확도 추가
        )
    
    # rscv.fit(xtrain, ytrain)
    rscv.fit(xtrain_scaled, ytrain)
  
    (회귀)변수 = rscv.best_estimator_    # 하이퍼 파라미터 최적화 값 저장
    rscv.best_params_  # 하이퍼 파라미터 최적화 값
    
    # rscv.cv_results_['mean_train_score']    # 하이퍼 파라미터 훈련 데이터 정확도
    # rscv.cv_results_['mean_test_score']     # 하이퍼 파라미터 검증 데이터 정확도
  
    # np.max(rscv.cv_results_['mean_train_score'])    # 하이퍼 파라미터 최적화 훈련 데이터 정확도
    # np.max(rscv.cv_results_['mean_test_score'])     # 하이퍼 파라미터 최적화 검증 데이터 정확도
    ```
  
>---
>### 하이퍼파라미터 최적화
>릿지／라쏘 - alpha 
>```
>train_score = []
>test_score = []
>
>alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
>for alpha in alpha_list:
>    ridge/lasso = Ridge/Lasso(alpha=alpha)
>    ridge/lasso.fit(xtrain_scaled, ytrain)
>    
>    train_score.append(ridge/lasso.score(xtrain_scaled, ytrain))
>    test_score.append(ridge/lasso.score(xtest_scaled, ytest))
>
># 시각화 - 선
>plt.plot(np.log10(alpha_list), train_score)
>plt.plot(np.log10(alpha_list), test_score)
>plt.xlabel('alpha')
>plt.ylabel('R^2')
>plt.show()
>```
>
>경사 하강법 - 적절한 학습률(0~1) ★
>```
>그리드 탐색??
>
>학습률: 학습 시킬 데이터 크기
>```


